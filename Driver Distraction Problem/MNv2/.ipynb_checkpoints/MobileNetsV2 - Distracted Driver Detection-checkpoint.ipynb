{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torchvision\n",
    "from torchvision import transforms, datasets\n",
    "from torch.utils.data import random_split\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "from torchvision.models import mobilenet_v2\n",
    "import torch.nn as nn\n",
    "import os\n",
    "import cv2\n",
    "import random\n",
    "import time\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x2320023f2f0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_rows, img_cols = 224, 224\n",
    "batch_size = 16\n",
    "random_state = 51\n",
    "torch.manual_seed(random_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_transform = transforms.Compose([\n",
    "        transforms.Resize((224,224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.5, 0.5, 0.5],\n",
    "                             std=[0.2, 0.2, 0.2])\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = 'C:/Users/Vedant/Desktop/ML_Assignment_201701076/Driver Distraction Problem/imgs/train'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = datasets.ImageFolder(root=PATH,transform=data_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "test_dataset,valid_dataset = random_split(val_dataset, [int(0.5*val_size), val_size - int(0.5*val_size)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag2class = {'0': 'normal driving','1': 'texting - right','2': 'talking on the phone - right','3': 'texting - left','4': 'talking on the phone - left','5': 'operating the radio','6': 'drinking','7': 'reaching behind','8': 'hair and makeup','9': 'talking to passenger'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(\n",
    "            train_dataset,  # The training samples.\n",
    "            sampler = RandomSampler(train_dataset), # Select batches randomly\n",
    "            batch_size = batch_size # Trains with this batch size.\n",
    "        )\n",
    "validation_dataloader = DataLoader(\n",
    "            valid_dataset, # The validation samples.\n",
    "            sampler = SequentialSampler(valid_dataset), # Pull out batches sequentially.\n",
    "            batch_size = batch_size # Evaluate with this batch size.\n",
    "        )\n",
    "test_dataloader = DataLoader(\n",
    "            test_dataset, # The validation samples.\n",
    "            sampler = SequentialSampler(test_dataset), # Pull out batches sequentially.\n",
    "            batch_size = batch_size # Evaluate with this batch size.\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = mobilenet_v2(pretrained=True, progress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.classifier = nn.Sequential(nn.Dropout(p=0.3),nn.Linear(1280,10),nn.Softmax(dim = 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MobileNetV2(\n",
       "  (features): Sequential(\n",
       "    (0): ConvBNReLU(\n",
       "      (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU6(inplace=True)\n",
       "    )\n",
       "    (1): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): ConvBNReLU(\n",
       "          (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (2): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): ConvBNReLU(\n",
       "          (0): Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): ConvBNReLU(\n",
       "          (0): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)\n",
       "          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (3): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): ConvBNReLU(\n",
       "          (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): ConvBNReLU(\n",
       "          (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)\n",
       "          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (4): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): ConvBNReLU(\n",
       "          (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): ConvBNReLU(\n",
       "          (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144, bias=False)\n",
       "          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (5): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): ConvBNReLU(\n",
       "          (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): ConvBNReLU(\n",
       "          (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
       "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (6): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): ConvBNReLU(\n",
       "          (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): ConvBNReLU(\n",
       "          (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
       "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (7): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): ConvBNReLU(\n",
       "          (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): ConvBNReLU(\n",
       "          (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False)\n",
       "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (8): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): ConvBNReLU(\n",
       "          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): ConvBNReLU(\n",
       "          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
       "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (9): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): ConvBNReLU(\n",
       "          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): ConvBNReLU(\n",
       "          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
       "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (10): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): ConvBNReLU(\n",
       "          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): ConvBNReLU(\n",
       "          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
       "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (11): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): ConvBNReLU(\n",
       "          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): ConvBNReLU(\n",
       "          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
       "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(384, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (12): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): ConvBNReLU(\n",
       "          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): ConvBNReLU(\n",
       "          (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)\n",
       "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (13): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): ConvBNReLU(\n",
       "          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): ConvBNReLU(\n",
       "          (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)\n",
       "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (14): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): ConvBNReLU(\n",
       "          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): ConvBNReLU(\n",
       "          (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=576, bias=False)\n",
       "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(576, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (15): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): ConvBNReLU(\n",
       "          (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): ConvBNReLU(\n",
       "          (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
       "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (16): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): ConvBNReLU(\n",
       "          (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): ConvBNReLU(\n",
       "          (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
       "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (17): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): ConvBNReLU(\n",
       "          (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): ConvBNReLU(\n",
       "          (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
       "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(960, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (18): ConvBNReLU(\n",
       "      (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU6(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (classifier): Sequential(\n",
       "    (0): Dropout(p=0.3, inplace=False)\n",
       "    (1): Linear(in_features=1280, out_features=10, bias=True)\n",
       "    (2): Softmax(dim=0)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Weigths are tranfers  \n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(filter(lambda p: p.requires_grad, model.parameters()), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import datetime\n",
    "\n",
    "def format_time(elapsed):\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0  Batch   100  of  1,122.  Elapsed: 0:00:29. Loss 0.20502993788523682\n",
      "Epoch 0  Batch   200  of  1,122.  Elapsed: 0:00:58. Loss 0.4094367754013143\n",
      "Epoch 0  Batch   300  of  1,122.  Elapsed: 0:01:28. Loss 0.613096521287466\n",
      "Epoch 0  Batch   400  of  1,122.  Elapsed: 0:01:56. Loss 0.815847549421477\n",
      "Epoch 0  Batch   500  of  1,122.  Elapsed: 0:02:25. Loss 1.0165098138459014\n",
      "Epoch 0  Batch   600  of  1,122.  Elapsed: 0:02:54. Loss 1.2133468801750007\n",
      "Epoch 0  Batch   700  of  1,122.  Elapsed: 0:03:23. Loss 1.4051454188351962\n",
      "Epoch 0  Batch   800  of  1,122.  Elapsed: 0:03:52. Loss 1.5916711011247413\n",
      "Epoch 0  Batch   900  of  1,122.  Elapsed: 0:04:22. Loss 1.7721538330144424\n",
      "Epoch 0  Batch 1,000  of  1,122.  Elapsed: 0:04:51. Loss 1.9497490210754136\n",
      "Epoch 0  Batch 1,100  of  1,122.  Elapsed: 0:05:20. Loss 2.1248469198663815\n",
      "  Accuracy: 0.52\n",
      "\n",
      "  Average training loss: 2.16\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.79\n",
      "  Validation Loss: 1.94\n",
      "Epoch 1  Batch   100  of  1,122.  Elapsed: 0:00:29. Loss 0.17215323756297854\n",
      "Epoch 1  Batch   200  of  1,122.  Elapsed: 0:00:58. Loss 0.34304741965258184\n",
      "Epoch 1  Batch   300  of  1,122.  Elapsed: 0:01:27. Loss 0.5133631013718943\n",
      "Epoch 1  Batch   400  of  1,122.  Elapsed: 0:01:55. Loss 0.6834150325380757\n",
      "Epoch 1  Batch   500  of  1,122.  Elapsed: 0:02:24. Loss 0.8538024282072955\n",
      "Epoch 1  Batch   600  of  1,122.  Elapsed: 0:02:52. Loss 1.0233639241752353\n",
      "Epoch 1  Batch   700  of  1,122.  Elapsed: 0:03:22. Loss 1.1922455340783227\n",
      "Epoch 1  Batch   800  of  1,122.  Elapsed: 0:03:50. Loss 1.3624824775306512\n",
      "Epoch 1  Batch   900  of  1,122.  Elapsed: 0:04:19. Loss 1.5306492713470934\n",
      "Epoch 1  Batch 1,000  of  1,122.  Elapsed: 0:04:48. Loss 1.6990551896486266\n",
      "Epoch 1  Batch 1,100  of  1,122.  Elapsed: 0:05:17. Loss 1.8672106126837977\n",
      "  Accuracy: 0.78\n",
      "\n",
      "  Average training loss: 1.90\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.80\n",
      "  Validation Loss: 1.89\n",
      "Epoch 2  Batch   100  of  1,122.  Elapsed: 0:00:29. Loss 0.1689807743949686\n",
      "Epoch 2  Batch   200  of  1,122.  Elapsed: 0:00:58. Loss 0.33766798582094026\n",
      "Epoch 2  Batch   300  of  1,122.  Elapsed: 0:01:27. Loss 0.5056495304201164\n",
      "Epoch 2  Batch   400  of  1,122.  Elapsed: 0:01:55. Loss 0.67381985206655\n",
      "Epoch 2  Batch   500  of  1,122.  Elapsed: 0:02:24. Loss 0.8422903305918987\n",
      "Epoch 2  Batch   600  of  1,122.  Elapsed: 0:02:53. Loss 1.0110317579140213\n",
      "Epoch 2  Batch   700  of  1,122.  Elapsed: 0:03:21. Loss 1.1798602566787053\n",
      "Epoch 2  Batch   800  of  1,122.  Elapsed: 0:03:49. Loss 1.3486732470797982\n",
      "Epoch 2  Batch   900  of  1,122.  Elapsed: 0:04:18. Loss 1.5164429297846693\n",
      "Epoch 2  Batch 1,000  of  1,122.  Elapsed: 0:04:47. Loss 1.6850223167269838\n",
      "Epoch 2  Batch 1,100  of  1,122.  Elapsed: 0:05:16. Loss 1.8540029003020915\n",
      "  Accuracy: 0.81\n",
      "\n",
      "  Average training loss: 1.89\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.83\n",
      "  Validation Loss: 1.89\n",
      "Epoch 3  Batch   100  of  1,122.  Elapsed: 0:00:29. Loss 0.16860947379453933\n",
      "Epoch 3  Batch   200  of  1,122.  Elapsed: 0:00:57. Loss 0.3362687633849295\n",
      "Epoch 3  Batch   300  of  1,122.  Elapsed: 0:01:25. Loss 0.5041229563388383\n",
      "Epoch 3  Batch   400  of  1,122.  Elapsed: 0:01:54. Loss 0.6716784863548483\n",
      "Epoch 3  Batch   500  of  1,122.  Elapsed: 0:02:21. Loss 0.8399772706728781\n",
      "Epoch 3  Batch   600  of  1,122.  Elapsed: 0:02:49. Loss 1.0081786528203152\n",
      "Epoch 3  Batch   700  of  1,122.  Elapsed: 0:03:17. Loss 1.1759170029252608\n",
      "Epoch 3  Batch   800  of  1,122.  Elapsed: 0:03:45. Loss 1.3435817357690576\n",
      "Epoch 3  Batch   900  of  1,122.  Elapsed: 0:04:13. Loss 1.5115079397398732\n",
      "Epoch 3  Batch 1,000  of  1,122.  Elapsed: 0:04:41. Loss 1.6796878783460607\n",
      "Epoch 3  Batch 1,100  of  1,122.  Elapsed: 0:05:09. Loss 1.8478045728117387\n",
      "  Accuracy: 0.84\n",
      "\n",
      "  Average training loss: 1.88\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.85\n",
      "  Validation Loss: 1.88\n",
      "Epoch 4  Batch   100  of  1,122.  Elapsed: 0:00:28. Loss 0.1671653634299144\n",
      "Epoch 4  Batch   200  of  1,122.  Elapsed: 0:00:56. Loss 0.3349932777477883\n",
      "Epoch 4  Batch   300  of  1,122.  Elapsed: 0:01:24. Loss 0.5026970619398855\n",
      "Epoch 4  Batch   400  of  1,122.  Elapsed: 0:01:52. Loss 0.6708476260809125\n",
      "Epoch 4  Batch   500  of  1,122.  Elapsed: 0:02:20. Loss 0.839062183192282\n",
      "Epoch 4  Batch   600  of  1,122.  Elapsed: 0:02:48. Loss 1.0071558731337495\n",
      "Epoch 4  Batch   700  of  1,122.  Elapsed: 0:03:16. Loss 1.17473627762361\n",
      "Epoch 4  Batch   800  of  1,122.  Elapsed: 0:03:44. Loss 1.3436792218748899\n",
      "Epoch 4  Batch   900  of  1,122.  Elapsed: 0:04:12. Loss 1.511746777038948\n",
      "Epoch 4  Batch 1,000  of  1,122.  Elapsed: 0:04:40. Loss 1.6793227216989173\n",
      "Epoch 4  Batch 1,100  of  1,122.  Elapsed: 0:05:08. Loss 1.8473689025948605\n",
      "  Accuracy: 0.86\n",
      "\n",
      "  Average training loss: 1.88\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.87\n",
      "  Validation Loss: 1.88\n",
      "Epoch 5  Batch   100  of  1,122.  Elapsed: 0:00:28. Loss 0.16847622989545744\n",
      "Epoch 5  Batch   200  of  1,122.  Elapsed: 0:00:56. Loss 0.33608023495597633\n",
      "Epoch 5  Batch   300  of  1,122.  Elapsed: 0:01:24. Loss 0.5032704422605654\n",
      "Epoch 5  Batch   400  of  1,122.  Elapsed: 0:01:52. Loss 0.6713013565051577\n",
      "Epoch 5  Batch   500  of  1,122.  Elapsed: 0:02:20. Loss 0.8392747895602874\n",
      "Epoch 5  Batch   600  of  1,122.  Elapsed: 0:02:48. Loss 1.0066913861429414\n",
      "Epoch 5  Batch   700  of  1,122.  Elapsed: 0:03:16. Loss 1.1746177015670056\n",
      "Epoch 5  Batch   800  of  1,122.  Elapsed: 0:03:43. Loss 1.3422494974790835\n",
      "Epoch 5  Batch   900  of  1,122.  Elapsed: 0:04:11. Loss 1.5101781814706092\n",
      "Epoch 5  Batch 1,000  of  1,122.  Elapsed: 0:04:39. Loss 1.6780535571290423\n",
      "Epoch 5  Batch 1,100  of  1,122.  Elapsed: 0:05:07. Loss 1.8459061586070613\n",
      "  Accuracy: 0.87\n",
      "\n",
      "  Average training loss: 1.88\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.87\n",
      "  Validation Loss: 1.88\n",
      "Epoch 6  Batch   100  of  1,122.  Elapsed: 0:00:28. Loss 0.1675257831546287\n",
      "Epoch 6  Batch   200  of  1,122.  Elapsed: 0:00:57. Loss 0.33499457226292545\n",
      "Epoch 6  Batch   300  of  1,122.  Elapsed: 0:01:25. Loss 0.5023975760116509\n",
      "Epoch 6  Batch   400  of  1,122.  Elapsed: 0:01:53. Loss 0.6704026405399071\n",
      "Epoch 6  Batch   500  of  1,122.  Elapsed: 0:02:22. Loss 0.8372889996212434\n",
      "Epoch 6  Batch   600  of  1,122.  Elapsed: 0:02:50. Loss 1.0053098021131572\n",
      "Epoch 6  Batch   700  of  1,122.  Elapsed: 0:03:18. Loss 1.172650341575371\n",
      "Epoch 6  Batch   800  of  1,122.  Elapsed: 0:03:46. Loss 1.3407985953723682\n",
      "Epoch 6  Batch   900  of  1,122.  Elapsed: 0:04:15. Loss 1.5082143625898583\n",
      "Epoch 6  Batch 1,000  of  1,122.  Elapsed: 0:04:42. Loss 1.6761156870932077\n",
      "Epoch 6  Batch 1,100  of  1,122.  Elapsed: 0:05:11. Loss 1.8441570289632216\n",
      "  Accuracy: 0.88\n",
      "\n",
      "  Average training loss: 1.88\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.89\n",
      "  Validation Loss: 1.88\n",
      "Epoch 7  Batch   100  of  1,122.  Elapsed: 0:00:29. Loss 0.1678603377571718\n",
      "Epoch 7  Batch   200  of  1,122.  Elapsed: 0:00:56. Loss 0.33512508741674574\n",
      "Epoch 7  Batch   300  of  1,122.  Elapsed: 0:01:24. Loss 0.5026994365634341\n",
      "Epoch 7  Batch   400  of  1,122.  Elapsed: 0:01:52. Loss 0.6704850966067663\n",
      "Epoch 7  Batch   500  of  1,122.  Elapsed: 0:02:20. Loss 0.837963813235203\n",
      "Epoch 7  Batch   600  of  1,122.  Elapsed: 0:02:48. Loss 1.0052144528497775\n",
      "Epoch 7  Batch   700  of  1,122.  Elapsed: 0:03:16. Loss 1.1734209192415397\n",
      "Epoch 7  Batch   800  of  1,122.  Elapsed: 0:03:44. Loss 1.3406137069703847\n",
      "Epoch 7  Batch   900  of  1,122.  Elapsed: 0:04:12. Loss 1.5077072554093631\n",
      "Epoch 7  Batch 1,000  of  1,122.  Elapsed: 0:04:39. Loss 1.676018524828654\n",
      "Epoch 7  Batch 1,100  of  1,122.  Elapsed: 0:05:07. Loss 1.8454795854826875\n",
      "  Accuracy: 0.89\n",
      "\n",
      "  Average training loss: 1.88\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.89\n",
      "  Validation Loss: 1.88\n",
      "Epoch 8  Batch   100  of  1,122.  Elapsed: 0:00:28. Loss 0.16747757351547213\n",
      "Epoch 8  Batch   200  of  1,122.  Elapsed: 0:00:56. Loss 0.3344519236083549\n",
      "Epoch 8  Batch   300  of  1,122.  Elapsed: 0:01:24. Loss 0.5014133547929094\n",
      "Epoch 8  Batch   400  of  1,122.  Elapsed: 0:01:51. Loss 0.6692208657290208\n",
      "Epoch 8  Batch   500  of  1,122.  Elapsed: 0:02:19. Loss 0.8364387061081681\n",
      "Epoch 8  Batch   600  of  1,122.  Elapsed: 0:02:47. Loss 1.004672074062939\n",
      "Epoch 8  Batch   700  of  1,122.  Elapsed: 0:03:14. Loss 1.1723273996788337\n",
      "Epoch 8  Batch   800  of  1,122.  Elapsed: 0:03:42. Loss 1.3397034031609587\n",
      "Epoch 8  Batch   900  of  1,122.  Elapsed: 0:04:10. Loss 1.5069127910489917\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8  Batch 1,000  of  1,122.  Elapsed: 0:04:38. Loss 1.67414114373253\n",
      "Epoch 8  Batch 1,100  of  1,122.  Elapsed: 0:05:05. Loss 1.8414954920291051\n",
      "  Accuracy: 0.90\n",
      "\n",
      "  Average training loss: 1.88\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.90\n",
      "  Validation Loss: 1.88\n",
      "Epoch 9  Batch   100  of  1,122.  Elapsed: 0:00:28. Loss 0.16696610350872318\n",
      "Epoch 9  Batch   200  of  1,122.  Elapsed: 0:00:56. Loss 0.33436164913330485\n",
      "Epoch 9  Batch   300  of  1,122.  Elapsed: 0:01:23. Loss 0.5025521217182995\n",
      "Epoch 9  Batch   400  of  1,122.  Elapsed: 0:01:51. Loss 0.6706965801550105\n",
      "Epoch 9  Batch   500  of  1,122.  Elapsed: 0:02:19. Loss 0.837803089576184\n",
      "Epoch 9  Batch   600  of  1,122.  Elapsed: 0:02:47. Loss 1.0054564048899686\n",
      "Epoch 9  Batch   700  of  1,122.  Elapsed: 0:03:14. Loss 1.1730051425264056\n",
      "Epoch 9  Batch   800  of  1,122.  Elapsed: 0:03:42. Loss 1.3403832013594275\n",
      "Epoch 9  Batch   900  of  1,122.  Elapsed: 0:04:10. Loss 1.5075570053807767\n",
      "Epoch 9  Batch 1,000  of  1,122.  Elapsed: 0:04:37. Loss 1.6759017463673882\n",
      "Epoch 9  Batch 1,100  of  1,122.  Elapsed: 0:05:05. Loss 1.8436359423154602\n",
      "  Accuracy: 0.91\n",
      "\n",
      "  Average training loss: 1.88\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.90\n",
      "  Validation Loss: 1.88\n",
      "Epoch 10  Batch   100  of  1,122.  Elapsed: 0:00:28. Loss 0.1670758335348119\n",
      "Epoch 10  Batch   200  of  1,122.  Elapsed: 0:00:56. Loss 0.33355939218279723\n",
      "Epoch 10  Batch   300  of  1,122.  Elapsed: 0:01:23. Loss 0.5007589963030687\n",
      "Epoch 10  Batch   400  of  1,122.  Elapsed: 0:01:51. Loss 0.6684100960452714\n",
      "Epoch 10  Batch   500  of  1,122.  Elapsed: 0:02:19. Loss 0.8359484281556917\n",
      "Epoch 10  Batch   600  of  1,122.  Elapsed: 0:02:47. Loss 1.003947038489017\n",
      "Epoch 10  Batch   700  of  1,122.  Elapsed: 0:03:14. Loss 1.171537005221355\n",
      "Epoch 10  Batch   800  of  1,122.  Elapsed: 0:03:42. Loss 1.3387054701114933\n",
      "Epoch 10  Batch   900  of  1,122.  Elapsed: 0:04:10. Loss 1.5056437561006257\n",
      "Epoch 10  Batch 1,000  of  1,122.  Elapsed: 0:04:37. Loss 1.6729001861842558\n",
      "Epoch 10  Batch 1,100  of  1,122.  Elapsed: 0:05:06. Loss 1.8408670619847303\n",
      "  Accuracy: 0.92\n",
      "\n",
      "  Average training loss: 1.88\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.90\n",
      "  Validation Loss: 1.88\n",
      "Epoch 11  Batch   100  of  1,122.  Elapsed: 0:00:28. Loss 0.1678644193472497\n",
      "Epoch 11  Batch   200  of  1,122.  Elapsed: 0:00:56. Loss 0.33533770313449934\n",
      "Epoch 11  Batch   300  of  1,122.  Elapsed: 0:01:24. Loss 0.5023068166246601\n",
      "Epoch 11  Batch   400  of  1,122.  Elapsed: 0:01:51. Loss 0.6688504946848075\n",
      "Epoch 11  Batch   500  of  1,122.  Elapsed: 0:02:19. Loss 0.8365411637301113\n",
      "Epoch 11  Batch   600  of  1,122.  Elapsed: 0:02:47. Loss 1.0045701579826614\n",
      "Epoch 11  Batch   700  of  1,122.  Elapsed: 0:03:15. Loss 1.1735588606453622\n",
      "Epoch 11  Batch   800  of  1,122.  Elapsed: 0:03:42. Loss 1.3407052707544622\n",
      "Epoch 11  Batch   900  of  1,122.  Elapsed: 0:04:11. Loss 1.508557570809349\n",
      "Epoch 11  Batch 1,000  of  1,122.  Elapsed: 0:04:38. Loss 1.676310155693435\n",
      "Epoch 11  Batch 1,100  of  1,122.  Elapsed: 0:05:06. Loss 1.8435766720304303\n",
      "  Accuracy: 0.92\n",
      "\n",
      "  Average training loss: 1.88\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.92\n",
      "  Validation Loss: 1.88\n",
      "Epoch 12  Batch   100  of  1,122.  Elapsed: 0:00:28. Loss 0.16679897665339996\n",
      "Epoch 12  Batch   200  of  1,122.  Elapsed: 0:00:56. Loss 0.3343383159866945\n",
      "Epoch 12  Batch   300  of  1,122.  Elapsed: 0:01:24. Loss 0.5021616560465086\n",
      "Epoch 12  Batch   400  of  1,122.  Elapsed: 0:01:52. Loss 0.6698968313807048\n",
      "Epoch 12  Batch   500  of  1,122.  Elapsed: 0:02:20. Loss 0.8369322165235904\n",
      "Epoch 12  Batch   600  of  1,122.  Elapsed: 0:02:47. Loss 1.0046640112863292\n",
      "Epoch 12  Batch   700  of  1,122.  Elapsed: 0:03:15. Loss 1.17191042842712\n",
      "Epoch 12  Batch   800  of  1,122.  Elapsed: 0:03:43. Loss 1.339497547311154\n",
      "Epoch 12  Batch   900  of  1,122.  Elapsed: 0:04:11. Loss 1.506126120447475\n",
      "Epoch 12  Batch 1,000  of  1,122.  Elapsed: 0:04:39. Loss 1.673898240036718\n",
      "Epoch 12  Batch 1,100  of  1,122.  Elapsed: 0:05:06. Loss 1.8417268759851575\n",
      "  Accuracy: 0.92\n",
      "\n",
      "  Average training loss: 1.88\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.93\n",
      "  Validation Loss: 1.88\n",
      "Epoch 13  Batch   100  of  1,122.  Elapsed: 0:00:28. Loss 0.16864436227370072\n",
      "Epoch 13  Batch   200  of  1,122.  Elapsed: 0:00:56. Loss 0.3350128616453705\n",
      "Epoch 13  Batch   300  of  1,122.  Elapsed: 0:01:23. Loss 0.5020821074325983\n",
      "Epoch 13  Batch   400  of  1,122.  Elapsed: 0:01:51. Loss 0.6697131625896257\n",
      "Epoch 13  Batch   500  of  1,122.  Elapsed: 0:02:19. Loss 0.8376488295680912\n",
      "Epoch 13  Batch   600  of  1,122.  Elapsed: 0:02:47. Loss 1.0050634154236466\n",
      "Epoch 13  Batch   700  of  1,122.  Elapsed: 0:03:14. Loss 1.1718743388240562\n",
      "Epoch 13  Batch   800  of  1,122.  Elapsed: 0:03:42. Loss 1.3388274002840175\n",
      "Epoch 13  Batch   900  of  1,122.  Elapsed: 0:04:09. Loss 1.5064578648132012\n",
      "Epoch 13  Batch 1,000  of  1,122.  Elapsed: 0:04:37. Loss 1.673983612800027\n",
      "Epoch 13  Batch 1,100  of  1,122.  Elapsed: 0:05:05. Loss 1.8416016885941042\n",
      "  Accuracy: 0.93\n",
      "\n",
      "  Average training loss: 1.88\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.93\n",
      "  Validation Loss: 1.88\n",
      "Epoch 14  Batch   100  of  1,122.  Elapsed: 0:00:28. Loss 0.16793118929905476\n",
      "Epoch 14  Batch   200  of  1,122.  Elapsed: 0:00:56. Loss 0.33527269507899427\n",
      "Epoch 14  Batch   300  of  1,122.  Elapsed: 0:01:23. Loss 0.5026824296901995\n",
      "Epoch 14  Batch   400  of  1,122.  Elapsed: 0:01:51. Loss 0.669793469808106\n",
      "Epoch 14  Batch   500  of  1,122.  Elapsed: 0:02:19. Loss 0.8376543247763486\n",
      "Epoch 14  Batch   600  of  1,122.  Elapsed: 0:02:46. Loss 1.0054505842467256\n",
      "Epoch 14  Batch   700  of  1,122.  Elapsed: 0:03:14. Loss 1.1728108847205014\n",
      "Epoch 14  Batch   800  of  1,122.  Elapsed: 0:03:42. Loss 1.3397617710890404\n",
      "Epoch 14  Batch   900  of  1,122.  Elapsed: 0:04:10. Loss 1.507656028138975\n",
      "Epoch 14  Batch 1,000  of  1,122.  Elapsed: 0:04:37. Loss 1.6742247658617355\n",
      "Epoch 14  Batch 1,100  of  1,122.  Elapsed: 0:05:05. Loss 1.8414284562469592\n",
      "  Accuracy: 0.93\n",
      "\n",
      "  Average training loss: 1.88\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.94\n",
      "  Validation Loss: 1.88\n",
      "Epoch 15  Batch   100  of  1,122.  Elapsed: 0:00:28. Loss 0.16738362352690808\n",
      "Epoch 15  Batch   200  of  1,122.  Elapsed: 0:00:55. Loss 0.33432890249021124\n",
      "Epoch 15  Batch   300  of  1,122.  Elapsed: 0:01:23. Loss 0.5004563011800116\n",
      "Epoch 15  Batch   400  of  1,122.  Elapsed: 0:01:51. Loss 0.6683605582106347\n",
      "Epoch 15  Batch   500  of  1,122.  Elapsed: 0:02:19. Loss 0.8351993733973851\n",
      "Epoch 15  Batch   600  of  1,122.  Elapsed: 0:02:46. Loss 1.00142564312447\n",
      "Epoch 15  Batch   700  of  1,122.  Elapsed: 0:03:14. Loss 1.1689070643800679\n",
      "Epoch 15  Batch   800  of  1,122.  Elapsed: 0:03:42. Loss 1.3372263401587379\n",
      "Epoch 15  Batch   900  of  1,122.  Elapsed: 0:04:10. Loss 1.5045211899514292\n",
      "Epoch 15  Batch 1,000  of  1,122.  Elapsed: 0:04:37. Loss 1.6719046897004208\n",
      "Epoch 15  Batch 1,100  of  1,122.  Elapsed: 0:05:05. Loss 1.8390164330681378\n",
      "  Accuracy: 0.94\n",
      "\n",
      "  Average training loss: 1.88\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.93\n",
      "  Validation Loss: 1.88\n",
      "Epoch 16  Batch   100  of  1,122.  Elapsed: 0:00:28. Loss 0.16610785569736664\n",
      "Epoch 16  Batch   200  of  1,122.  Elapsed: 0:00:56. Loss 0.3328107164080343\n",
      "Epoch 16  Batch   300  of  1,122.  Elapsed: 0:01:24. Loss 0.49989264087880997\n",
      "Epoch 16  Batch   400  of  1,122.  Elapsed: 0:01:51. Loss 0.6680384354795365\n",
      "Epoch 16  Batch   500  of  1,122.  Elapsed: 0:02:20. Loss 0.835273252026495\n",
      "Epoch 16  Batch   600  of  1,122.  Elapsed: 0:02:47. Loss 1.0022058970362957\n",
      "Epoch 16  Batch   700  of  1,122.  Elapsed: 0:03:15. Loss 1.1689812982996093\n",
      "Epoch 16  Batch   800  of  1,122.  Elapsed: 0:03:43. Loss 1.3358023199593108\n",
      "Epoch 16  Batch   900  of  1,122.  Elapsed: 0:04:11. Loss 1.5035658156808047\n",
      "Epoch 16  Batch 1,000  of  1,122.  Elapsed: 0:04:39. Loss 1.6703498650362147\n",
      "Epoch 16  Batch 1,100  of  1,122.  Elapsed: 0:05:07. Loss 1.8379959407762199\n",
      "  Accuracy: 0.94\n",
      "\n",
      "  Average training loss: 1.87\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.93\n",
      "  Validation Loss: 1.88\n",
      "Epoch 17  Batch   100  of  1,122.  Elapsed: 0:00:28. Loss 0.16660064150730344\n",
      "Epoch 17  Batch   200  of  1,122.  Elapsed: 0:00:56. Loss 0.334233697085457\n",
      "Epoch 17  Batch   300  of  1,122.  Elapsed: 0:01:23. Loss 0.5014874799153809\n",
      "Epoch 17  Batch   400  of  1,122.  Elapsed: 0:01:52. Loss 0.6687081788737931\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17  Batch   500  of  1,122.  Elapsed: 0:02:19. Loss 0.8350342173202365\n",
      "Epoch 17  Batch   600  of  1,122.  Elapsed: 0:02:47. Loss 1.0021486605339933\n",
      "Epoch 17  Batch   700  of  1,122.  Elapsed: 0:03:15. Loss 1.1696001147841388\n",
      "Epoch 17  Batch   800  of  1,122.  Elapsed: 0:03:43. Loss 1.3369904597175313\n",
      "Epoch 17  Batch   900  of  1,122.  Elapsed: 0:04:11. Loss 1.5041770692815117\n",
      "Epoch 17  Batch 1,000  of  1,122.  Elapsed: 0:04:39. Loss 1.671555227243114\n",
      "Epoch 17  Batch 1,100  of  1,122.  Elapsed: 0:05:06. Loss 1.838291147175957\n",
      "  Accuracy: 0.95\n",
      "\n",
      "  Average training loss: 1.88\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.94\n",
      "  Validation Loss: 1.88\n",
      "Epoch 18  Batch   100  of  1,122.  Elapsed: 0:00:28. Loss 0.16706105997643156\n",
      "Epoch 18  Batch   200  of  1,122.  Elapsed: 0:00:56. Loss 0.3353356670143345\n",
      "Epoch 18  Batch   300  of  1,122.  Elapsed: 0:01:24. Loss 0.5021366130646963\n",
      "Epoch 18  Batch   400  of  1,122.  Elapsed: 0:01:51. Loss 0.6695125684168675\n",
      "Epoch 18  Batch   500  of  1,122.  Elapsed: 0:02:19. Loss 0.836641702634978\n",
      "Epoch 18  Batch   600  of  1,122.  Elapsed: 0:02:47. Loss 1.003643108136726\n",
      "Epoch 18  Batch   700  of  1,122.  Elapsed: 0:03:15. Loss 1.1712635243427731\n",
      "Epoch 18  Batch   800  of  1,122.  Elapsed: 0:03:42. Loss 1.337983391714181\n",
      "Epoch 18  Batch   900  of  1,122.  Elapsed: 0:04:10. Loss 1.5052495672103556\n",
      "Epoch 18  Batch 1,000  of  1,122.  Elapsed: 0:04:38. Loss 1.6725850179658641\n",
      "Epoch 18  Batch 1,100  of  1,122.  Elapsed: 0:05:06. Loss 1.8409226219071848\n",
      "  Accuracy: 0.94\n",
      "\n",
      "  Average training loss: 1.88\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.93\n",
      "  Validation Loss: 1.88\n",
      "Epoch 19  Batch   100  of  1,122.  Elapsed: 0:00:28. Loss 0.1673070161525366\n",
      "Epoch 19  Batch   200  of  1,122.  Elapsed: 0:00:56. Loss 0.3345561891316093\n",
      "Epoch 19  Batch   300  of  1,122.  Elapsed: 0:01:24. Loss 0.5022899316168936\n",
      "Epoch 19  Batch   400  of  1,122.  Elapsed: 0:01:51. Loss 0.6700002188776054\n",
      "Epoch 19  Batch   500  of  1,122.  Elapsed: 0:02:19. Loss 0.8381963032239687\n",
      "Epoch 19  Batch   600  of  1,122.  Elapsed: 0:02:47. Loss 1.0053862183489264\n",
      "Epoch 19  Batch   700  of  1,122.  Elapsed: 0:03:15. Loss 1.1726552702738853\n",
      "Epoch 19  Batch   800  of  1,122.  Elapsed: 0:03:43. Loss 1.3405588966214934\n",
      "Epoch 19  Batch   900  of  1,122.  Elapsed: 0:04:10. Loss 1.508583638013578\n",
      "Epoch 19  Batch 1,000  of  1,122.  Elapsed: 0:04:38. Loss 1.6753626978971103\n",
      "Epoch 19  Batch 1,100  of  1,122.  Elapsed: 0:05:06. Loss 1.8420548063973479\n",
      "  Accuracy: 0.95\n",
      "\n",
      "  Average training loss: 1.88\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.93\n",
      "  Validation Loss: 1.88\n",
      "Epoch 20  Batch   100  of  1,122.  Elapsed: 0:00:28. Loss 0.16815055415379598\n",
      "Epoch 20  Batch   200  of  1,122.  Elapsed: 0:00:56. Loss 0.3358094913224273\n",
      "Epoch 20  Batch   300  of  1,122.  Elapsed: 0:01:24. Loss 0.5037935685346471\n",
      "Epoch 20  Batch   400  of  1,122.  Elapsed: 0:01:52. Loss 0.6705354664627456\n",
      "Epoch 20  Batch   500  of  1,122.  Elapsed: 0:02:20. Loss 0.8374801291926447\n",
      "Epoch 20  Batch   600  of  1,122.  Elapsed: 0:02:47. Loss 1.0039507827444298\n",
      "Epoch 20  Batch   700  of  1,122.  Elapsed: 0:03:15. Loss 1.171340269200942\n",
      "Epoch 20  Batch   800  of  1,122.  Elapsed: 0:03:43. Loss 1.3381082007591738\n",
      "Epoch 20  Batch   900  of  1,122.  Elapsed: 0:04:10. Loss 1.505081500492844\n",
      "Epoch 20  Batch 1,000  of  1,122.  Elapsed: 0:04:38. Loss 1.6737185877062208\n",
      "Epoch 20  Batch 1,100  of  1,122.  Elapsed: 0:05:06. Loss 1.8406750438685087\n",
      "  Accuracy: 0.95\n",
      "\n",
      "  Average training loss: 1.88\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.94\n",
      "  Validation Loss: 1.88\n",
      "Epoch 21  Batch   100  of  1,122.  Elapsed: 0:00:28. Loss 0.1671937038979216\n",
      "Epoch 21  Batch   200  of  1,122.  Elapsed: 0:00:56. Loss 0.3344944168327114\n",
      "Epoch 21  Batch   300  of  1,122.  Elapsed: 0:01:24. Loss 0.5023264305901824\n",
      "Epoch 21  Batch   400  of  1,122.  Elapsed: 0:01:52. Loss 0.6684568065373018\n",
      "Epoch 21  Batch   500  of  1,122.  Elapsed: 0:02:19. Loss 0.8352350656361504\n",
      "Epoch 21  Batch   600  of  1,122.  Elapsed: 0:02:47. Loss 1.0027612302180067\n",
      "Epoch 21  Batch   700  of  1,122.  Elapsed: 0:03:15. Loss 1.1705763368683064\n",
      "Epoch 21  Batch   800  of  1,122.  Elapsed: 0:03:43. Loss 1.3375890770272987\n",
      "Epoch 21  Batch   900  of  1,122.  Elapsed: 0:04:10. Loss 1.5061107951477037\n",
      "Epoch 21  Batch 1,000  of  1,122.  Elapsed: 0:04:38. Loss 1.6746157248389912\n",
      "Epoch 21  Batch 1,100  of  1,122.  Elapsed: 0:05:06. Loss 1.8419638940144776\n",
      "  Accuracy: 0.95\n",
      "\n",
      "  Average training loss: 1.88\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.94\n",
      "  Validation Loss: 1.88\n",
      "Epoch 22  Batch   100  of  1,122.  Elapsed: 0:00:27. Loss 0.16704928003742944\n",
      "Epoch 22  Batch   200  of  1,122.  Elapsed: 0:00:56. Loss 0.333972681758663\n",
      "Epoch 22  Batch   300  of  1,122.  Elapsed: 0:01:23. Loss 0.5014777910263143\n",
      "Epoch 22  Batch   400  of  1,122.  Elapsed: 0:01:51. Loss 0.6681454655533381\n",
      "Epoch 22  Batch   500  of  1,122.  Elapsed: 0:02:18. Loss 0.8350701438339424\n",
      "Epoch 22  Batch   600  of  1,122.  Elapsed: 0:02:46. Loss 1.002733726127475\n",
      "Epoch 22  Batch   700  of  1,122.  Elapsed: 0:03:13. Loss 1.170509443882315\n",
      "Epoch 22  Batch   800  of  1,122.  Elapsed: 0:03:41. Loss 1.3381128354718554\n",
      "Epoch 22  Batch   900  of  1,122.  Elapsed: 0:04:08. Loss 1.5057460883509455\n",
      "Epoch 22  Batch 1,000  of  1,122.  Elapsed: 0:04:36. Loss 1.6735149434968537\n",
      "Epoch 22  Batch 1,100  of  1,122.  Elapsed: 0:05:03. Loss 1.8399310355305458\n",
      "  Accuracy: 0.96\n",
      "\n",
      "  Average training loss: 1.88\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.94\n",
      "  Validation Loss: 1.88\n",
      "Epoch 23  Batch   100  of  1,122.  Elapsed: 0:00:28. Loss 0.1672460478257368\n",
      "Epoch 23  Batch   200  of  1,122.  Elapsed: 0:00:56. Loss 0.3349890782233865\n",
      "Epoch 23  Batch   300  of  1,122.  Elapsed: 0:01:23. Loss 0.5019050296827646\n",
      "Epoch 23  Batch   400  of  1,122.  Elapsed: 0:01:51. Loss 0.668518796005351\n",
      "Epoch 23  Batch   500  of  1,122.  Elapsed: 0:02:19. Loss 0.8359232216595328\n",
      "Epoch 23  Batch   600  of  1,122.  Elapsed: 0:02:46. Loss 1.0041925554394509\n",
      "Epoch 23  Batch   700  of  1,122.  Elapsed: 0:03:14. Loss 1.1709302557555963\n",
      "Epoch 23  Batch   800  of  1,122.  Elapsed: 0:03:41. Loss 1.3383931996775609\n",
      "Epoch 23  Batch   900  of  1,122.  Elapsed: 0:04:09. Loss 1.5054844041984137\n",
      "Epoch 23  Batch 1,000  of  1,122.  Elapsed: 0:04:37. Loss 1.6731692946743413\n",
      "Epoch 23  Batch 1,100  of  1,122.  Elapsed: 0:05:04. Loss 1.840722470891242\n",
      "  Accuracy: 0.95\n",
      "\n",
      "  Average training loss: 1.88\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.94\n",
      "  Validation Loss: 1.88\n",
      "Epoch 24  Batch   100  of  1,122.  Elapsed: 0:00:28. Loss 0.1675059111148066\n",
      "Epoch 24  Batch   200  of  1,122.  Elapsed: 0:00:55. Loss 0.334479648586688\n",
      "Epoch 24  Batch   300  of  1,122.  Elapsed: 0:01:23. Loss 0.5012194599272308\n",
      "Epoch 24  Batch   400  of  1,122.  Elapsed: 0:01:51. Loss 0.6678280926005725\n",
      "Epoch 24  Batch   500  of  1,122.  Elapsed: 0:02:19. Loss 0.8346912132014991\n",
      "Epoch 24  Batch   600  of  1,122.  Elapsed: 0:02:46. Loss 1.0025146951012434\n",
      "Epoch 24  Batch   700  of  1,122.  Elapsed: 0:03:14. Loss 1.1685587295236435\n",
      "Epoch 24  Batch   800  of  1,122.  Elapsed: 0:03:41. Loss 1.335928036142373\n",
      "Epoch 24  Batch   900  of  1,122.  Elapsed: 0:04:09. Loss 1.5029578575475968\n",
      "Epoch 24  Batch 1,000  of  1,122.  Elapsed: 0:04:37. Loss 1.6705042498846956\n",
      "Epoch 24  Batch 1,100  of  1,122.  Elapsed: 0:05:04. Loss 1.8359388648507429\n",
      "  Accuracy: 0.96\n",
      "\n",
      "  Average training loss: 1.87\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.95\n",
      "  Validation Loss: 1.88\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 25\n",
    "\n",
    "training_stats = []\n",
    "total_eval_accuracy = 0\n",
    "total_train_accuracy = 0\n",
    "best_avg_val_accuracy = 0\n",
    "\n",
    "for epoch in range(0,EPOCHS):  # loop over the dataset multiple times\n",
    "    \n",
    "    t0 = time.time()\n",
    "    running_loss = 0.0\n",
    "    total_train_accuracy = 0\n",
    "    \n",
    "    for i,batch in enumerate(train_dataloader):\n",
    "        \n",
    "        if i % 100 == 0 and not i == 0:\n",
    "            elapsed = format_time(time.time() - t0)\n",
    "            print('Epoch {}  Batch {:>5,}  of  {:>5,}.  Elapsed: {:}. Loss {}'.format(epoch,i, len(train_dataloader), elapsed,\n",
    "                                                                                       avg_train_loss))\n",
    "        \n",
    "        inputs, labels = batch\n",
    "        \n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = model(inputs.cuda())\n",
    "        \n",
    "        loss = criterion(outputs,labels.type(torch.LongTensor).cuda())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        avg_train_loss = running_loss / len(train_dataloader)\n",
    "        outputs = outputs.detach().cpu().numpy()\n",
    "        label_ids = labels.detach().to('cpu').numpy()\n",
    "\n",
    "        total_train_accuracy += flat_accuracy(outputs, label_ids)\n",
    "        \n",
    "\n",
    "    avg_train_accuracy = total_train_accuracy / len(train_dataloader)\n",
    "    print(\"  Accuracy: {0:.2f}\".format(avg_train_accuracy))\n",
    "    \n",
    "    print(\"\")\n",
    "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"Running Validation...\")\n",
    "\n",
    "    model.eval()\n",
    " \n",
    "    total_eval_accuracy = 0\n",
    "    total_eval_loss = 0\n",
    "    nb_eval_steps = 0\n",
    "\n",
    "    for batch in validation_dataloader:\n",
    "        \n",
    "        inputs, labels = batch\n",
    "        with torch.no_grad():        \n",
    "            outputs = model(inputs.cuda())\n",
    "            \n",
    "        # Accumulate the validation loss.\n",
    "        loss = criterion(outputs,labels.type(torch.LongTensor).cuda())\n",
    "        total_eval_loss += loss.item()\n",
    "\n",
    "        # Move logits and labels to CPU\n",
    "        outputs = outputs.detach().cpu().numpy()\n",
    "        label_ids = labels.detach().to('cpu').numpy()\n",
    "\n",
    "        total_eval_accuracy += flat_accuracy(outputs, label_ids)\n",
    "        \n",
    "\n",
    "    avg_val_accuracy = total_eval_accuracy / len(validation_dataloader)\n",
    "    print(\"  Accuracy: {0:.2f}\".format(avg_val_accuracy))\n",
    "\n",
    "    # Calculate the average loss over all of the batches.\n",
    "    avg_val_loss = total_eval_loss / len(validation_dataloader)\n",
    "    print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n",
    "    \n",
    "    training_stats.append(\n",
    "        {\n",
    "            'epoch': epoch + 1,\n",
    "            'Training Loss': avg_train_loss,\n",
    "            'Valid. Loss': avg_val_loss,\n",
    "            'Valid. Accur.': avg_val_accuracy,\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    if best_avg_val_accuracy < avg_val_accuracy:\n",
    "        best_avg_val_accuracy = avg_val_accuracy\n",
    "        if epoch > 10:\n",
    "            torch.save(model.state_dict(), 'checkpoint_'+str(epoch)+'.pth')\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_stats = pd.DataFrame(training_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MobileNetV2(\n",
       "  (features): Sequential(\n",
       "    (0): ConvBNReLU(\n",
       "      (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU6(inplace=True)\n",
       "    )\n",
       "    (1): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): ConvBNReLU(\n",
       "          (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (2): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): ConvBNReLU(\n",
       "          (0): Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): ConvBNReLU(\n",
       "          (0): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)\n",
       "          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (3): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): ConvBNReLU(\n",
       "          (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): ConvBNReLU(\n",
       "          (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)\n",
       "          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (4): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): ConvBNReLU(\n",
       "          (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): ConvBNReLU(\n",
       "          (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144, bias=False)\n",
       "          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (5): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): ConvBNReLU(\n",
       "          (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): ConvBNReLU(\n",
       "          (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
       "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (6): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): ConvBNReLU(\n",
       "          (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): ConvBNReLU(\n",
       "          (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
       "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (7): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): ConvBNReLU(\n",
       "          (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): ConvBNReLU(\n",
       "          (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False)\n",
       "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (8): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): ConvBNReLU(\n",
       "          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): ConvBNReLU(\n",
       "          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
       "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (9): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): ConvBNReLU(\n",
       "          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): ConvBNReLU(\n",
       "          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
       "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (10): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): ConvBNReLU(\n",
       "          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): ConvBNReLU(\n",
       "          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
       "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (11): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): ConvBNReLU(\n",
       "          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): ConvBNReLU(\n",
       "          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
       "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(384, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (12): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): ConvBNReLU(\n",
       "          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): ConvBNReLU(\n",
       "          (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)\n",
       "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (13): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): ConvBNReLU(\n",
       "          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): ConvBNReLU(\n",
       "          (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)\n",
       "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (14): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): ConvBNReLU(\n",
       "          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): ConvBNReLU(\n",
       "          (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=576, bias=False)\n",
       "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(576, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (15): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): ConvBNReLU(\n",
       "          (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): ConvBNReLU(\n",
       "          (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
       "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (16): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): ConvBNReLU(\n",
       "          (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): ConvBNReLU(\n",
       "          (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
       "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (17): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): ConvBNReLU(\n",
       "          (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): ConvBNReLU(\n",
       "          (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
       "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(960, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (18): ConvBNReLU(\n",
       "      (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU6(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (classifier): Sequential(\n",
       "    (0): Dropout(p=0.3, inplace=False)\n",
       "    (1): Linear(in_features=1280, out_features=10, bias=True)\n",
       "    (2): Softmax(dim=0)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PATH = 'C:/Users/Vedant/Desktop/ML_Assignment_201701076/checkpoint_24.pth'\n",
    "model.load_state_dict(torch.load(PATH))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Accuracy: 0.94\n",
      "  Test Loss: 1.87\n"
     ]
    }
   ],
   "source": [
    "total_test_accuracy = 0\n",
    "total_test_loss = 0\n",
    "\n",
    "for i, batch in enumerate(test_dataloader):\n",
    "\n",
    "    inputs, labels = batch\n",
    "    with torch.no_grad():        \n",
    "        outputs = model(inputs.cuda())\n",
    "\n",
    "    # Accumulate the validation loss.\n",
    "    loss = criterion(outputs,labels.type(torch.LongTensor).cuda())\n",
    "    total_test_loss += loss.item()\n",
    "\n",
    "    # Move logits and labels to CPU\n",
    "    outputs = outputs.detach().cpu().numpy()\n",
    "    label_ids = labels.detach().to('cpu').numpy()\n",
    "\n",
    "    total_test_accuracy += flat_accuracy(outputs, label_ids)\n",
    "\n",
    "\n",
    "avg_test_accuracy = total_test_accuracy / len(test_dataloader)\n",
    "print(\"  Accuracy: {0:.2f}\".format(avg_test_accuracy))\n",
    "\n",
    "# Calculate the average loss over all of the batches.\n",
    "avg_test_loss = total_test_loss / len(test_dataloader)\n",
    "print(\"  Test Loss: {0:.2f}\".format(avg_test_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
